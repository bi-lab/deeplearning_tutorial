{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep Reinforcement Learning Tutorial with TensorFlow\n",
    "\n",
    "본 튜토리얼은 Tensorflow 환경에서 DeeP Reinforcement Learning에 대한 기초적인 실습을 하기 위한 자료이다. 첫 번째 파트에서는 tensorflow에 대한 기초적인 실습과 MLP 예제를 다루어보고, 두 번째 파트에서는 Karpathy가 오픈소스로 공개한 Deep Reinforcement Learning 예제를 분석하고 직접 수정해보는 시간을 갖는다.\n",
    "\n",
    "\n",
    "The code and comments are written by Dong-Hyun Kwak (imcomking@gmail.com)\n",
    "\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. TensorFlow\n",
    "\n",
    "TensorFlow 이하, tf는 구글 주도하에 초기 개발되고, 현재 오픈소스로 공개되어 널리 쓰이고 있는 분산 기계학습(딥러닝)을 위한 라이브러리이다. Computation Graph를 사용한 Theano의 장점을 그대로 살려 automatic gradient 계산이 가능하고, Spark처럼 분산 클라우드 컴퓨팅 환경에서 동작하기 위한 아키텍처로 설계되었다. 그리고 당연히 GPU와 CPU 연산 모두 지원한다.\n",
    "\n",
    "Computation Graph는 tf를 이해하는 데 가장 중요한 핵심이다. 말그대로 어떤 모델을 우리가 코딩하게 되면, 이 모델의 계산 과정이 일련의 node와 edge들로 연결된 graph를 이루게 되는데 이것이 바로 Computation Graph이다.\n",
    "\n",
    "그래서 성공적으로 모델을 작성하게 되면 아래와 같은 Computation Graph가 만들어 진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"tensorboard_mlp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 기본적인 tf의 문법과 형식을 익히기 위해, MLP 예제 코드를 실습해보자.\n",
    "\n",
    "### Multi-Layer Perceptron\n",
    "Multi-Layer Perceptron, 이하 MLP는 다음과 같은 구조를 가진 모델이다. Convolutional Neural Networks와 달리 굉장히 layer간의 연결이 빽빽하게 가득 차 있어, dense layer 혹은 fully connected layer라는 이름으로도 불리고 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"mlp.png\">\n",
    "(출처: http://blog.refu.co/?p=931)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf를 이용해서 MNIST 데이터를 MLP로 분류하는 코드를 작성해보자.\n",
    "가장 먼저 사용할 라이브러리를 import 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 실습에서 사용할 MNIST 데이터를 다음과 같이 다운로드 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# download the mnist data.\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 mnist 데이터를 저장시킬 x와 y_target 변수를 선언해야한다.\n",
    "tf.placeholder는 우리가 원하는 데이터를 Computation Graph에 입력시켜주는 역할을 하는 변수이다. 즉 input 을 받기 위한 변수라고 생각할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder is used for feeding data.\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) # none represents variable length of dimension. 784 is the dimension of MNIST data.\n",
    "y_target = tf.placeholder(\"float\", shape=[None, 10]) # shape argument is optional, but this is useful to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 넣을 변수를 생성했으니, 이제 실제 Neural Network에서 사용할 변수들을 생성하고, 이들을 이용해 Computation Graph를 그려본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all the variables are allocated in GPU memory\n",
    "W1 = tf.Variable(tf.zeros([784, 256]))      # create (784 * 256) matrix\n",
    "b1 = tf.Variable(tf.zeros([256]))           # create (1 * 256) vector\n",
    "weighted_summation1 = tf.matmul(x, W1) + b1 # compute --> weighted summation\n",
    "h1 = tf.sigmoid( weighted_summation1 )      # compute --> sigmoid(weighted summation)\n",
    "\n",
    "# Repeat again\n",
    "W2 = tf.Variable(tf.zeros([256, 10]))        # create (256 * 10) matrix\n",
    "b2 = tf.Variable(tf.zeros([10]))             # create (1 * 10) vector\n",
    "weighted_summation2 = tf.matmul(h1, W2) + b2 # compute --> weighted summation\n",
    "y = tf.nn.softmax(weighted_summation2)       # compute classification --> softmax(weighted summation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정까지 완료된 경우, 변수 y는 3층짜리 Neural Network(MLP)의 연산 결과가 저장되도록 되어있다. 이제 이 MLP를 학습하고 실행시켜보자.\n",
    "\n",
    "먼저 tf에는 Session이라는 개념이 있다. 이는 간단히 말해 하나의 Computation Graph가 실행되기 위한 계산 단위를 의미한다. 즉 하나의 session에는 보통 하나의 graph가 담겨있다. 이를 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session() # open a session which is a envrionment of computation graph.\n",
    "sess.run(tf.initialize_all_variables())# initialize the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 tf.initialize_all_variables() 는 앞서 생성했던 변수들을 사용하기 위해서 반드시 실행해야하는 초기화 단계이다. 이 연산은 session에 의해서 실행되므로, 이와 같이 sess.run() 함수를 이용해 실행시켜준다.\n",
    "\n",
    "이제 MLP를 학습시키기 위한 미분 계산과 같은 수학적인 연산들을 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the Loss function\n",
    "cross_entropy = -tf.reduce_sum(y_target*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross entorpy는 MLP의 분류 모델에서 사용하는 에러함수이다. 간단히 말해 이 cross entropy는 MLP가 예측한 y 값이 실제 데이터와 다른 정도를 확률적으로 측정한다고 생각해볼 수 있다.\n",
    "\n",
    "MLP가 학습 되기위해서는 이 에러함수가 최대한 작은 값을 내도록 만들어야한다. 그래서 이 에러함수를 각각의 변수들로 편미분하여 gradient를 계산하고, 에러가 최소가 되는 변수값을 찾아가는 것이 바로 MLP의 학습 알고리즘이다.\n",
    "\n",
    "이를 구현하려면 원래 미분 후 에러가 줄어드는 방향으로 변수를 이동시키는 과정등을 직접 코딩해야하지만, 자동 미분을 제공하는 tf에서는 단 한줄로 구현할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define optimization algorithm\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습 알고리즘의 구현이 끝났고, MLP가 잘 학습하고 있는지 성능을 측정하기 위한 정확도 계산을 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "# correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "# tf.cast() : changes true -> 1 / false -> 0\n",
    "# tf.reduce_mean() : calculate the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct_prediction 는 뭔가 수식이 복잡해 보이지만 사실 매우 간단한 계산이다. tf.argmax()는 y 벡터 중에서 가장 값이 큰 index를 알려주는 함수이다. 즉 모델이 예측한 class, y와 실제 데이터에 labeling된 y_target을 비교하여 같으면 true, 다르면 false를 내도록 계산한 것이다.\n",
    "\n",
    "그리고 accuracy는 앞서 계산한 true/false 리스트를 1과 0으로 변환한 뒤, 이를 평균낸 것이다.\n",
    "\n",
    "자 그러면 이제 필요한 모든 Computation Graph를 정의하였다. 이제 이를 session을 이용하여 실행만 시키면 된다.\n",
    "\n",
    "\n",
    "우선은 Ctrl + Enter를 눌러 다음 코드를 실행 시켜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy: 0.940\n",
      "step 500, training accuracy: 0.980\n",
      "step 1000, training accuracy: 0.950\n",
      "step 1500, training accuracy: 0.920\n",
      "step 2000, training accuracy: 1.000\n",
      "step 2500, training accuracy: 0.980\n",
      "step 3000, training accuracy: 0.970\n",
      "step 3500, training accuracy: 0.970\n",
      "step 4000, training accuracy: 0.990\n",
      "step 4500, training accuracy: 0.950\n",
      "step 5000, training accuracy: 0.970\n",
      "test accuracy: 0.9563\n"
     ]
    }
   ],
   "source": [
    "# training the MLP\n",
    "for i in range(5001): # minibatch iteraction\n",
    "    batch = mnist.train.next_batch(100) # minibatch size\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y_target: batch[1]}) # feed data into placeholder x, y_target\n",
    "\n",
    "    if i%500 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x:batch[0], y_target: batch[1]})\n",
    "        print \"step %d, training accuracy: %.3f\"%(i, train_accuracy)\n",
    "\n",
    "# for given x, y_target data set\n",
    "print  \"test accuracy: %g\"% sess.run(accuracy, feed_dict={x: mnist.test.images, y_target: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 5000번의 minibatch iteraction을 실행하고, 500번 마다 학습 정확도를 측정, 그리고 마지막에는 테스트 정확도를 측정하고 있다.\n",
    "\n",
    "코드는 간단한 구조이다. for 문이 전체 iteration을 실행하고, 맨 처음 가져왔던 mnist 데이터를 100개씩 가져와서 place_holder에 넣어준다. 그리고 sess.run()을 통해 위에서 정의했던 학습 알고리즘과 정확도 계산을 실행시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리가 만든 MLP의 구조를 직접 눈으로 확인해보자.\n",
    "\n",
    "### TensorBoard 설정하기\n",
    "TensorFlow는 TensorBoard라는 매우 강력한 visualization tool을 제공한다. 이를 사용하면 웹브라우저 형태로 사용자가 모델의 구조를 눈으로 확인하고, 파라미터 값의 변화를 살펴보는 등의 직관적인 분석이 가능하다.\n",
    "\n",
    "이를 활용해 방금 만들었던 MLP를 분석해보자. 그러려면 다음의 사항을 반영해 코드를 수정하여야 한다.\n",
    "\n",
    "* **변수들의 이름 지어주기**\n",
    "\n",
    "* **변수들의 Summary 생성**\n",
    "\n",
    "* **변수들의 Summary 기록**\n",
    "\n",
    "아래의 코드는 위의 3가지 사항을 모두 반영하고, MLP 코드를 하나의 함수로 정리한 코드이다. 세세한 차이는 위에서 우리가 짰던 코드와 비교를 하면 파악이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy: 0.120\n",
      "step 500, training accuracy: 0.580\n",
      "step 1000, training accuracy: 0.880\n",
      "step 1500, training accuracy: 0.900\n",
      "step 2000, training accuracy: 0.940\n",
      "step 2500, training accuracy: 0.930\n",
      "step 3000, training accuracy: 0.940\n",
      "step 3500, training accuracy: 0.860\n",
      "step 4000, training accuracy: 0.910\n",
      "step 4500, training accuracy: 0.920\n",
      "step 5000, training accuracy: 0.970\n",
      "test accuracy: 0.9201\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def MLP():\n",
    "    # download the mnist data.\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True) \n",
    "\n",
    "\n",
    "    # placeholder is used for feeding data.\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784], name = 'x') # none represents variable length of dimension. 784 is the dimension of MNIST data.\n",
    "    y_target = tf.placeholder(\"float\", shape=[None, 10], name = 'y_target') # shape argument is optional, but this is useful to debug.\n",
    "\n",
    "\n",
    "    # all the variables are allocated in GPU memory\n",
    "    W1 = tf.Variable(tf.zeros([784, 256]), name = 'W1')   # create (784 * 256) matrix\n",
    "    b1 = tf.Variable(tf.zeros([256]), name = 'b1')        # create (1 * 256) vector\n",
    "    h1 = tf.sigmoid(tf.matmul(x, W1) + b1, name = 'h1')   # compute --> sigmoid(weighted summation)\n",
    "\n",
    "    # Repeat again\n",
    "    W2 = tf.Variable(tf.zeros([256, 10]), name = 'W2')     # create (256 * 10) matrix\n",
    "    b2 = tf.Variable(tf.zeros([10]), name = 'b2')          # create (1 * 10) vector\n",
    "    y = tf.nn.softmax(tf.matmul(h1, W2) + b2, name = 'y')  # compute classification --> softmax(weighted summation)\n",
    "\n",
    "    # Create Session\n",
    "    sess = tf.Session() # open a session which is a envrionment of computation graph.\n",
    "    sess.run(tf.initialize_all_variables())# initialize the variables\n",
    "\n",
    "\n",
    "\n",
    "    # define the Loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
    "\n",
    "\n",
    "    # define optimization algorithm\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "    # correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # tf.cast() : changes true -> 1 / false -> 0\n",
    "    # tf.reduce_mean() : calculate the mean\n",
    "\n",
    "\n",
    "\n",
    "    # create summary of parameters\n",
    "    tf.histogram_summary('weights_1', W1)\n",
    "    tf.histogram_summary('weights_2', W2)\n",
    "    tf.histogram_summary('y', y)\n",
    "    tf.scalar_summary('cross_entropy', cross_entropy)\n",
    "    merged = tf.merge_all_summaries()\n",
    "    summary_writer = tf.train.SummaryWriter(\"/tmp/mlp4\" , sess.graph_def)\n",
    "\n",
    "\n",
    "    # training the MLP\n",
    "    for i in range(5001): # minibatch iteraction\n",
    "        batch = mnist.train.next_batch(100) # minibatch size\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_target: batch[1]}) # placeholder's none length is replaced by i:i+100 indexes\n",
    "\n",
    "        if i%500 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={x:batch[0], y_target: batch[1]})\n",
    "            print \"step %d, training accuracy: %.3f\"%(i, train_accuracy)\n",
    "\n",
    "            # calculate the summary and write.\n",
    "            summary = sess.run(merged, feed_dict={x:batch[0], y_target: batch[1]})\n",
    "            summary_writer.add_summary(summary , i)\n",
    "\n",
    "    # for given x, y_target data set\n",
    "    print  \"test accuracy: %g\"% sess.run(accuracy, feed_dict={x: mnist.test.images, y_target: mnist.test.labels})\n",
    "\n",
    "MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard 실행하기\n",
    "위와 같이 코드를 수정했다면, 이제 리눅스 shell로 이동한 후, tensorboard를 실행시킨다.\n",
    "\n",
    "\n",
    "tensorboard --logdir=/tmp/mlp\n",
    "\n",
    "\n",
    "(만약 위 명령어 실행시 문제가 생기는 경우 다음을 실행)\n",
    "<br>cd tensorflow/tensorflow/tensorboard\n",
    "<br>python tensorboard.py --logdir=/tmp/mlp\n",
    "\n",
    "\n",
    "그다음 [docker_default_IP]:6006/#graphs 에 접속하면 아래와 같은 그림을 볼 수 있다.\n",
    "<br>(ex) http://192.168.99.100:6006/#graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"mlp_total.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Deep Reinforcement Learing\n",
    "\n",
    "Deep Reinforcement Learning이란, 기존의 강화학습에서 Q function을 딥러닝으로 근사하는 모델을 의미한다. 대표적으로 구글 Deep Mind의 Atari와 최근에 화제가 된 AlphaGo 역시 이 Deep Reinforcement Learning 응용의 한가지이다.\n",
    "\n",
    "이번 파트에서는 Deep Reinforcement Learning을 이용해서 간단한 2차원 게임을 플레이하고, reward로 부터 스스로 학습하는 Kaparthy의 오픈소스 예제를 실습해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Deep RL\" style=\"border-width:0\" width=\"600\" src=\"example.gif?raw=true\" />\n",
    "\n",
    "\n",
    "(출처: https://github.com/nivwusquorum/tensorflow-deepq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning, 이하 RL은 supervised learning과 달리 데이터에 대한 정확한 정답을 받지 않고, 내가 한 행동에 대한 reward feedback 만으로 학습을 수행하는 알고리즘이다. 이를 강화학습이라 부르며, 이것을 수행하는 가장 대표적인 알고리즘으로 Q-Learning 이 있다.\n",
    "\n",
    "RL을 이해하는 것은 매우 많은 공부를 필요로 하기 때문에, 우선 2D게임을 예로 들어 아주 기본적인 개념만 살펴본다.\n",
    "\n",
    "* **State**: 게임에서의 각 물체들의 위치, 속도, 벽과의 거리 등을 의미한다.\n",
    "* **Action**: 게임을 플레이하는 주인공의 행동을 의미한다. 여기서는 4가지 방향에 대한 움직임이 이에 해당한다.\n",
    "* **Reward**: 게임을 플레이하면서 받는 score. 여기서는 초록색을 먹으면 +1 , 빨간색을 먹으면 -1을 점수로 받는다.\n",
    "* **Value**: 해당 action 또는 state가 미래에 얼마나 큰 reward를 가져올 지에 대한 예측값.\n",
    "* **Policy**: 주인공이 현재의 게임 State에서 Reward를 최대한 얻기 위해 Action을 선택하는 전략. 한마디로 [게임을 플레이하는 방법]자체를 의미한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, HumanController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Settings\n",
    "\n",
    "이제 우리가 원하는 게임 환경을 설정하고, 적절한 reward와 object의 개수 및 observation 을 조절한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/drl\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    'objects': [\n",
    "        'friend',\n",
    "        'enemy',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'hero':   'yellow',\n",
    "        'friend': 'green',\n",
    "        'enemy':  'red',\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'friend': 0.1,\n",
    "        'enemy': -0.1,\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (700,500),\n",
    "    'hero_initial_position': [400, 300],\n",
    "    'hero_initial_speed':    [0,   0],\n",
    "    \"maximum_speed\":         [50, 50],\n",
    "    \"object_radius\": 10.0,\n",
    "    \"num_objects\": {\n",
    "        \"friend\" : 25,\n",
    "        \"enemy\" :  25,\n",
    "    },\n",
    "    \"num_observation_lines\" : 32, # the number of antennas\n",
    "    \"observation_line_length\": 240., # the length of antennas\n",
    "    \"tolerable_distance_to_wall\": 50, \n",
    "    \"wall_distance_penalty\":  -0.0, # if the hero is close to wall, that receives penalty\n",
    "    \"delta_v\": 50 # speed value\n",
    "}\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "# this LOG_DIR is used for tensorboard\n",
    "LOG_DIR = \"/tmp/drl\"\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Architecture\n",
    "\n",
    "이제 Q function을 근사하기 위한 딥러닝 모델을 만들어보자. 이번 예제에서는 위에서 보았던 4층짜리 MLP를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tf_rl.utils import base_name\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, input_sizes, output_size, scope):\n",
    "        \"\"\"Cretes a neural network layer.\"\"\"\n",
    "        if type(input_sizes) != list:\n",
    "            input_sizes = [input_sizes]\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        self.scope       = scope or \"Layer\"\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            self.Ws = []\n",
    "            for input_idx, input_size in enumerate(input_sizes):\n",
    "                W_name = \"W_%d\" % (input_idx,)\n",
    "                W_initializer =  tf.random_uniform_initializer(\n",
    "                        -1.0 / math.sqrt(input_size), 1.0 / math.sqrt(input_size))\n",
    "                W_var = tf.get_variable(W_name, (input_size, output_size), initializer=W_initializer)\n",
    "                self.Ws.append(W_var)\n",
    "            self.b = tf.get_variable(\"b\", (output_size,), initializer=tf.constant_initializer(0))\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        if type(xs) != list:\n",
    "            xs = [xs]\n",
    "        assert len(xs) == len(self.Ws), \\\n",
    "                \"Expected %d input vectors, got %d\" % (len(self.Ws), len(xs))\n",
    "        with tf.variable_scope(self.scope):\n",
    "            return sum([tf.matmul(x, W) for x, W in zip(xs, self.Ws)]) + self.b\n",
    "\n",
    "    def variables(self):\n",
    "        return [self.b] + self.Ws\n",
    "\n",
    "    def copy(self, scope=None):\n",
    "        scope = scope or self.scope + \"_copy\"\n",
    "\n",
    "        with tf.variable_scope(scope) as sc:\n",
    "            for v in self.variables():\n",
    "                tf.get_variable(base_name(v), v.get_shape(),\n",
    "                        initializer=lambda x,dtype=tf.float32: v.initialized_value())\n",
    "            sc.reuse_variables()\n",
    "            return Layer(self.input_sizes, self.output_size, scope=sc)\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, input_sizes, hiddens, nonlinearities, scope=None, given_layers=None):\n",
    "        self.input_sizes = input_sizes\n",
    "        # observation is 5 features(distance of each object and X,Y speed) of closest 32 object with hero(friend, enemy, wall) + 2 hero's own speed X,Y\n",
    "        # ==> 5*32 + 2 = 162 features about the game\n",
    "        self.hiddens = hiddens\n",
    "        self.input_nonlinearity, self.layer_nonlinearities = nonlinearities[0], nonlinearities[1:]\n",
    "        self.scope = scope or \"MLP\"\n",
    "\n",
    "        assert len(hiddens) == len(nonlinearities), \\\n",
    "                \"Number of hiddens must be equal to number of nonlinearities\"\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            if given_layers is not None:\n",
    "                self.input_layer = given_layers[0]\n",
    "                self.layers      = given_layers[1:]\n",
    "            else:\n",
    "                self.input_layer = Layer(input_sizes, hiddens[0], scope=\"input_layer\") # 135 -> 200\n",
    "                self.layers = []\n",
    "\n",
    "                for l_idx, (h_from, h_to) in enumerate(zip(hiddens[:-1], hiddens[1:])): # hiddens == [200, 200, 4], so this mean, swifting the index by 1\n",
    "                    # (200, 200) , (200,4)\n",
    "                    self.layers.append(Layer(h_from, h_to, scope=\"hidden_layer_%d\" % (l_idx,)))\n",
    "                    # this has 4 layers\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        if type(xs) != list:\n",
    "            xs = [xs]\n",
    "        with tf.variable_scope(self.scope):\n",
    "            hidden = self.input_nonlinearity(self.input_layer(xs))\n",
    "            for layer, nonlinearity in zip(self.layers, self.layer_nonlinearities):\n",
    "                hidden = nonlinearity(layer(hidden))\n",
    "            return hidden\n",
    "\n",
    "    def variables(self):\n",
    "        res = self.input_layer.variables()\n",
    "        for layer in self.layers:\n",
    "            res.extend(layer.variables())\n",
    "        return res\n",
    "\n",
    "    def copy(self, scope=None):\n",
    "        scope = scope or self.scope + \"_copy\"\n",
    "        nonlinearities = [self.input_nonlinearity] + self.layer_nonlinearities\n",
    "        given_layers = [self.input_layer.copy()] + [layer.copy() for layer in self.layers]\n",
    "        return MLP(self.input_sizes, self.hiddens, nonlinearities, scope=scope,\n",
    "                given_layers=given_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make an Agent\n",
    "\n",
    "이제 Discrete Deep Q learning 알고리즘이 이 게임을 플레이하면서 학습을 하도록 agent로 설정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow business - it is always good to reset a graph before creating a new controller.\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This little guy will let us run tensorboard\n",
    "#      tensorboard --logdir [LOG_DIR]\n",
    "journalist = tf.train.SummaryWriter(LOG_DIR)\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden\n",
    "# layers\n",
    "brain = MLP([g.observation_size,], [200, 200, g.num_actions], \n",
    "            [tf.tanh, tf.tanh, tf.identity])\n",
    "\n",
    "# The optimizer to use. Here we use RMSProp as recommended\n",
    "# by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = DiscreteDeepQ(g.observation_size, g.num_actions, brain, optimizer, session,\n",
    "                                   discount_rate=0.99, exploration_period=5000, max_experience=10000, \n",
    "                                   store_every_nth=4, train_every_nth=4,\n",
    "                                   summary_writer=journalist)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)\n",
    "# graph was not available when journalist was created  \n",
    "journalist.add_graph(session.graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play the Game\n",
    "\n",
    "실제로 게임을 플레이하면서 강화학습을 하는 과정을 지켜본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<?xml version=\"1.0\"?>\n",
       "\n",
       "<svg height=\"600\" width=\"720\" >\n",
       "\n",
       " <g style=\"fill-opacity:1.0; stroke:black;\n",
       "\n",
       "  stroke-width:1;\">\n",
       "\n",
       "  <rect x=\"10\" y=\"10\" height=\"500\"\n",
       "\n",
       "        width=\"700\" style=\"fill:none;\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"435\" y2=\"449\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"431\" y2=\"496\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"417\" y2=\"541\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"395\" y2=\"582\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"365\" y2=\"619\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"328\" y2=\"649\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"287\" y2=\"671\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"242\" y2=\"684\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"195\" y2=\"689\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"148\" y2=\"684\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"103\" y2=\"671\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"62\" y2=\"649\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"25\" y2=\"619\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"-3\" y2=\"582\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"-26\" y2=\"541\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"-39\" y2=\"496\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"-44\" y2=\"449\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"-39\" y2=\"402\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"-26\" y2=\"357\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"-3\" y2=\"316\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"25\" y2=\"279\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"62\" y2=\"249\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"103\" y2=\"227\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"148\" y2=\"214\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"195\" y2=\"209\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"242\" y2=\"214\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"287\" y2=\"227\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"328\" y2=\"249\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"365\" y2=\"279\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"395\" y2=\"316\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"417\" y2=\"357\" />\n",
       "\n",
       "  <line x1=\"195\" y1=\"449\" x2=\"431\" y2=\"402\" />\n",
       "\n",
       "  <circle cx=\"167\" cy=\"122\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"488\" cy=\"487\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"46\" cy=\"167\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"569\" cy=\"469\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"616\" cy=\"198\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"533\" cy=\"441\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"573\" cy=\"371\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"569\" cy=\"294\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"41\" cy=\"68\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"21\" cy=\"90\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"403\" cy=\"307\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"244\" cy=\"435\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"374\" cy=\"186\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"472\" cy=\"446\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"515\" cy=\"438\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"434\" cy=\"341\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"68\" cy=\"238\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"431\" cy=\"77\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"26\" cy=\"162\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"67\" cy=\"254\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"576\" cy=\"68\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"118\" cy=\"487\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"399\" cy=\"354\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"312\" cy=\"254\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"497\" cy=\"293\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"258\" cy=\"311\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"153\" cy=\"139\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"111\" cy=\"58\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"233\" cy=\"124\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"323\" cy=\"255\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"597\" cy=\"109\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"266\" cy=\"271\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"228\" cy=\"422\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"475\" cy=\"132\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"426\" cy=\"303\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"682\" cy=\"476\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"649\" cy=\"229\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"103\" cy=\"481\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"423\" cy=\"267\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"648\" cy=\"28\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"588\" cy=\"431\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"230\" cy=\"263\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"188\" cy=\"328\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"648\" cy=\"72\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"427\" cy=\"81\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"433\" cy=\"24\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"684\" cy=\"189\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"438\" cy=\"165\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"458\" cy=\"417\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"627\" cy=\"264\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"195\" cy=\"449\" r=\"10\"\n",
       "\n",
       "          style=\"fill:yellow;\" />\n",
       "\n",
       "  <text x=\"10\" y=\"535\" font-size=\"15\">\n",
       "\n",
       "   fps = 107.2\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"555\" font-size=\"15\">\n",
       "\n",
       "   nearest wall = 50.5\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"575\" font-size=\"15\">\n",
       "\n",
       "   reward       = -0.0\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"595\" font-size=\"15\">\n",
       "\n",
       "   objects eaten => enemy: 5, friend: 1\n",
       "\n",
       "  </text>\n",
       "\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<tf_rl.utils.svg.Scene instance at 0x7fe4940c33b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = True\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 20\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "    \n",
    "try:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        simulate(simulation=g,\n",
    "                 controller=current_controller,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=0.001,\n",
    "                 save_path=None)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contest!\n",
    "자 이제 위의 여러가지 환경 설정과 모델의 구조를 바꾸어 enemy:friend 의 차이가 최대한 많이 나도록 나의 agent를 학습시켜본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
